{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94292d3b-ca63-4b4c-823b-9dca4d4a4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9059a39-fabd-4219-937c-d25433a3df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size,d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) #initializes embeddings to be learned to training process\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) #normalize the variance of the embeddings\n",
    "\n",
    "\n",
    "\n",
    "#if feature=2i, sin(pos/1000^(2i/d_model), if feature=2i+k, cos(pos/1000^(2i/d_model)\n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self,seq_length,d_model,Pdrop):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(Pdrop)\n",
    "        #if you add self before, then it gets learned as a parameter for the model\n",
    "        pos = torch.arange(0,seq_length).unsqueeze(1) #pos --> seq_length_pos x 1, adds an extra dimension\n",
    "        div_term = torch.exp(torch.arange(0,d_model,2)*-np.log(1000)/d_model) #div term --> 1 x div_term_i\n",
    "        #so there is a different pe value for each token (pos) and feature (2i,2i+1)\n",
    "        pe = torch.zeros((seq_length, d_model))\n",
    "        pe[:,0::2] = torch.sin(pos*div_term)\n",
    "        pe[:,1::2] = torch.cos(pos*div_term)\n",
    "        pe = pe.unsqueeze(0) # --> (1, seq, d_model)\n",
    "        self.register_buffer('positional_encoder',pe) #self.positional_encoder, buffer is not a learnable parameter\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + (self.positional_encoder[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epsilon = 1**-10 #so not divide by 0\n",
    "        self.a = nn.Parameter(torch.ones(1)) #1 dimension\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) #1 dimension\n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) #keep all dimensions\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "\n",
    "        return self.a*(x-mean)/(std+self.epsilon)+self.bias\n",
    "\n",
    "\n",
    "#add non linearity\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,d_model,Pdrop, d_ff=None):\n",
    "        if d_ff == None:\n",
    "            d_ff = d_model*4\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model,d_ff)\n",
    "        self.dropout= nn.Dropout(Pdrop)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.dropout(F.relu(self.linear1(x)))\n",
    "        return self.linear2(x)\n",
    "\n",
    "\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self,LayerNormalization, Pdrop):\n",
    "        super(AddAndNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(Pdrop)\n",
    "        self.LayerNormalization = LayerNormalization()\n",
    "    def forward(self,x,sublayer):\n",
    "        return self.LayerNormalization(x+self.dropout(sublayer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ae560a-1747-40fd-bac5-7d2b8e7ba064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7581, -0.5262, -0.5710,  0.8651],\n",
       "         [-1.0470, -1.1174, -0.2363, -1.7936],\n",
       "         [ 1.0244, -0.3452,  0.5874, -0.5634]],\n",
       "\n",
       "        [[-2.3001, -1.5569, -1.1608, -0.3298],\n",
       "         [-1.2177, -1.5458, -0.7360, -1.8968],\n",
       "         [ 2.9878,  0.5898,  1.5703, -0.2662]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same computation overhead as self attention but more semantic representations\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, Pdrop):\n",
    "        super().__init__()\n",
    "        if d_model % h != 0:\n",
    "            raise ValueError()\n",
    "\n",
    "        d_k=d_v=d_model/h\n",
    "        self.h = h #num_heads\n",
    "        self.d_model = d_model #length of embeddings\n",
    "        self.d_k = self.d_v = int(self.d_model / self.h)\n",
    "\n",
    "        self.dropout= nn.Dropout(Pdrop)\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model,d_model)\n",
    "        self.W_K = nn.Linear(d_model,d_model)\n",
    "        self.W_V = nn.Linear(d_model,d_model)\n",
    "\n",
    "        self.W_O = nn.Linear(d_model,d_model)\n",
    "        \n",
    "    \n",
    "        \n",
    "    def forward(self,Q,K,V, mask):\n",
    "        batch_size, q_seq_length, d_model = Q.size()\n",
    "        batch_size, kv_seq_length, d_model = K.size()\n",
    "\n",
    "        queries = self.W_Q(Q)\n",
    "        keys = self.W_K(K)\n",
    "        values = self.W_V(V)\n",
    "\n",
    "        queries = queries.reshape(batch_size, q_seq_length, self.h, self.d_k) #(batch size, seq_length, d_model) ==> (batch size, seq_length, h, d_k)\n",
    "        keys = keys.reshape(batch_size, kv_seq_length, self.h, self.d_k)\n",
    "        values = values.reshape(batch_size, kv_seq_length, self.h, self.d_v)\n",
    "\n",
    "        queries = queries.transpose(1,2) #(batch size, h, seq_length, d_k), flips the dims 1 and 2\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        scores = torch.matmul(queries, keys.transpose(-2,-1))/self.d_k**.5\n",
    "\n",
    "        scores = scores.masked_fill_(mask==0, -1*10**10) #mask values that are equal to 0 with -10^10 bc e^-inf=0 in softmax will be equal to 0\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        attention = self.dropout(attention) #dropout after activation function\n",
    "            \n",
    "        weighted = torch.matmul(attention, values)\n",
    "        concat = weighted.reshape(batch_size, q_seq_length, d_model)\n",
    "        out = self.W_O(concat) #W_O\n",
    "        return out\n",
    "        \n",
    "x = torch.tensor([\n",
    "        [[1., 2., 3., 4.],\n",
    "         [2., 3., 4., 5.],\n",
    "         [3., 4., 5., 6.]],\n",
    "\n",
    "        [[4., 3., 2., 1.],\n",
    "         [5., 4., 3., 2.],\n",
    "         [6., 5., 4., 3.]]\n",
    "])\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(1,size,size),diagonal=1).type(torch.int) #diagonal mask so can't see tokens after it when making prediction\n",
    "    return mask == 0\n",
    "mha = MultiHeadAttention(d_model=4,h=2,Pdrop=.1)\n",
    "Q,K,V = x,x,x\n",
    "mha.forward(Q,K,V,causal_mask(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3661375-e8af-4d23-9a7a-68fa51817e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,h,Pdrop):\n",
    "        super().__init__()\n",
    "        self.MHA1 = MultiHeadAttention(d_model,h,Pdrop)\n",
    "        self.FFN = FeedForwardNetwork(d_model, Pdrop, d_ff=d_model*4)\n",
    "        self.residual = nn.ModuleList([AddAndNorm(LayerNormalization, Pdrop) for x in range(2)])\n",
    "                                      \n",
    "    def forward(self,x,mask):\n",
    "       x = self.residual[0](x,self.MHA1(x,x,x,mask))\n",
    "       x = self.residual[1](x,self.FFN(x))\n",
    "       return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,N,d_model,h,Pdrop):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(d_model,h,Pdrop) for x in range(N)])\n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09cef2f7-c777-4266-8b83-e390eeb7391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model,h,Pdrop):\n",
    "        super().__init__()\n",
    "        self.MHA2 = MultiHeadAttention(d_model,h,Pdrop)\n",
    "        self.MHA3 = MultiHeadAttention(d_model,h,Pdrop)\n",
    "        self.FFN = FeedForwardNetwork(d_model, Pdrop, d_ff=d_model*4)\n",
    "        self.residual = nn.ModuleList([AddAndNorm(LayerNormalization, Pdrop) for x in range(3)])\n",
    "    def forward(self,x,encoder_output,decoder_mask, encoder_mask):\n",
    "        x = self.residual[0](x,self.MHA2(x,x,x,decoder_mask))\n",
    "        x = self.residual[1](x,self.MHA3(x,encoder_output,encoder_output,encoder_mask))\n",
    "        x = self.residual[1](x,self.FFN(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,N,d_model,h,Pdrop):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderBlock(d_model,h,Pdrop) for x in range(N)])\n",
    "    def forward(self,x,encoder_output,decoder_mask, encoder_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,encoder_output,decoder_mask, encoder_mask)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c94713c-3c6e-4208-b270-a2aa631a55f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super(ProjectionLayer, self).__init__()\n",
    "        self.proj = nn.Linear(d_model,vocab_size)\n",
    "    def forward(self,x):\n",
    "        return torch.log_softmax(self.proj(x),dim=-1) #softmax along the vocab size dim --> batch_size x seq_length, vocab_size (vocab_probabilities)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037a5b00-8f7c-4dd1-a2fe-625303ed2665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 10\n",
    "torch.triu(torch.ones(1,size,size),diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eab0a84-871b-40bf-8995-dc81ef6a1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,N,src_vocab_size,tgt_vocab_size, src_seq_length, tgt_seq_length, d_model,h,Pdrop):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(N,d_model,h,Pdrop) #this is a nn.module so it runs forward\n",
    "        self.decoder = Decoder(N,d_model,h,Pdrop)\n",
    "        self.src_embeddings = Embeddings(src_vocab_size, d_model)\n",
    "        self.tgt_embeddings = Embeddings(tgt_vocab_size, d_model)\n",
    "        self.src_positional_encoding = Positional_Encoding(src_seq_length,d_model,Pdrop)\n",
    "        self.tgt_positional_encoding = Positional_Encoding(tgt_seq_length,d_model,Pdrop)\n",
    "        self.projection_layer = ProjectionLayer(tgt_vocab_size,d_model)\n",
    "\n",
    "    def encode(self,encoder_input, encoder_mask):\n",
    "        #encoder_input is tokens\n",
    "        x = self.src_embeddings(encoder_input)\n",
    "        x = self.src_positional_encoding(x)\n",
    "        x = self.encoder(x, encoder_mask)\n",
    "        return x\n",
    "        \n",
    "    def decode(self,decoder_input, encoder_output, decoder_mask, encoder_mask):\n",
    "        #decoder_input is tokens\n",
    "        x = self.tgt_embeddings(decoder_input)\n",
    "        x = self.tgt_positional_encoding(x)\n",
    "        x = self.decoder(x,encoder_output, decoder_mask, encoder_mask)\n",
    "        return x\n",
    "        \n",
    "    def project(self,x):\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "\n",
    "# t = Transformer()\n",
    "encoder_input = torch.tensor([2, 0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "decoder_input = torch.tensor([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ff593b3-2ed0-4596-ab02-a442e414b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process dataset\n",
    "class ProcessDataset(Dataset):\n",
    "    def __init__(self,data, tokenizer_src, tokenizer_tgt,seq_length, lang1, lang2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.lang1 = lang1\n",
    "        self.lang2 = lang2\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "        \n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index):\n",
    "        pair = self.data[index]\n",
    "        src_txt = pair['translation'][self.lang1]\n",
    "        tgt_txt = pair['translation'][self.lang2]\n",
    "        \n",
    "        encoder_input_tokens = self.tokenizer_src.encode(src_txt).ids\n",
    "        decoder_input_tokens = self.tokenizer_tgt.encode(tgt_txt).ids\n",
    "        \n",
    "        \n",
    "        encoder_num_padding_tokens = self.seq_length - len(encoder_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        decoder_num_padding_tokens = self.seq_length - len(decoder_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "        \n",
    "        if encoder_num_padding_tokens < 0 or decoder_num_padding_tokens < 0: #check if sentence is longer than seq_length - special tokens\n",
    "            raise ValueError('Sentence is too long')\n",
    "         \n",
    "        #get encoder input w/ special tokens\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(encoder_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "            self.eos_token, # Inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * encoder_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # get decoder input w/ special token SOS\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token \n",
    "                torch.tensor(decoder_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        \n",
    "        )\n",
    "        \n",
    "        # create a label, which is just the correct tgt output (decoder_input_tokens + EOS + padding)\n",
    "        #different than decoder_input because no SOS and has EOS before padding\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(decoder_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token \n",
    "                torch.tensor([self.pad_token] * decoder_num_padding_tokens, dtype = torch.int64) # adding padding tokens (same # as decoder input because replace SOS with EOS)\n",
    "                \n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input, \n",
    "            #unsqueeze twice, makes it 1,1,seq_length, which makes sense because it gives different mask for each token in the sequence which is in a batch\n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), #unmask everything that is not a padding token, ie. the entire encoder_input is unmasked\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), #mask everything that is a padding token and also use causal mask\n",
    "            'label': label,\n",
    "            'src_text': src_txt,\n",
    "            'tgt_text': tgt_txt\n",
    "        }    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0494eb1-8753-4ffc-8ae4-2fb4adf11727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Validation():\n",
    "    def run_prediction(self,encoder_input, encoder_mask):\n",
    "        max_length=10\n",
    "        sos_token = self.tokenizer_tgt.token_to_id('[SOS]') #sos token\n",
    "        eos_token = self.tokenizer_tgt.token_to_id('[EOS]')\n",
    "        encoder_output = self.model.encode(encoder_input,encoder_mask)\n",
    "        decoder_input = torch.empty(1,1).fill_(sos_token).type_as(encoder_input).to(device)\n",
    "        while True:\n",
    "            if decoder_input.size(1) == max_length: break\n",
    "            decoder_mask = causal_mask(decoder_input.size(1))\n",
    "            decoder_output = self.model.decode(decoder_input, encoder_output,decoder_mask, encoder_mask) #seq_length x d_model\n",
    "            probabilities = self.model.project(decoder_output[:,-1]) #batch_size x seq_length, vocab_size\n",
    "            max_probability, next_word = torch.max(probabilities, dim=1) #\n",
    "            decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "            if next_word == eos_token:\n",
    "                break\n",
    "            \n",
    "        return decoder_input.squeeze(0)\n",
    "\n",
    "    def run_validation(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_iterator = tqdm(self.val_dataloader, desc = f'Validation')\n",
    "            for batch in batch_iterator:\n",
    "                encoder_input = batch['encoder_input']\n",
    "                encoder_mask = batch['encoder_mask']\n",
    "                result = self.run_prediction(encoder_input, encoder_mask)\n",
    "                label = batch['label']\n",
    "                src_text = batch['src_txt']\n",
    "                tgt_txt = batch['tgt_text']\n",
    "                predicted_txt = self.tokenizer_tgt.decode(result.detach().cpu().numpy())\n",
    "                print(src_text, tgt_txt,predicted_txt)\n",
    "\n",
    "    def predict(self, src_txt):\n",
    "        sos_token = torch.tensor([self.tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        eos_token = torch.tensor([self.tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        pad_token = torch.tensor([self.tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            encoder_input_tokens = self.tokenizer_src.encode(src_txt).ids\n",
    "            encoder_num_padding_tokens = self.config[\"src_seq_length\"] - len(encoder_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "            #get encoder input w/ special tokens\n",
    "            encoder_input = torch.cat(\n",
    "                [\n",
    "                sos_token,\n",
    "                torch.tensor(encoder_input_tokens, dtype = torch.int64), \n",
    "                eos_token,\n",
    "                torch.tensor([pad_token] * encoder_num_padding_tokens, dtype = torch.int64) \n",
    "                ]\n",
    "            ).unsqueeze(0)\n",
    "            encoder_mask = (encoder_input != pad_token).unsqueeze(0).unsqueeze(0).int()\n",
    "            result = self.run_prediction(encoder_input, encoder_mask)\n",
    "            predicted_txt = self.tokenizer_tgt.decode(result.detach().cpu().numpy())\n",
    "            print(f\"input: {src_txt}, output: {predicted_txt}\")\n",
    "            return predicted_txt\n",
    "            \n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4995632f-16a0-4ef3-a787-bca1ac9c10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self,config):\n",
    "        self.tokenizer_file = \"tokenizer {}.json\"\n",
    "        self.config = config\n",
    "        self.train_dataloader, self.val_dataloader, self.tokenizer_src, self.tokenizer_tgt = self.get_data()\n",
    "    def get_sentences(self,data,lang):\n",
    "        for pair in data:\n",
    "            yield pair['translation'][lang] #generator function, executes until yield, state is saved, then resumes from same spot\n",
    "            #allows to process data without loading all to memory\n",
    "            \n",
    "    def build_tokenizer(self,data,lang):\n",
    "        tokenizer_path = Path(self.tokenizer_file.format(lang))\n",
    "        newTokenizer = True #not Path.exists(tokenizer_path)\n",
    "        if newTokenizer: \n",
    "            tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) #word level tokenizer\n",
    "            tokenizer.pre_tokenizer = Whitespace() #based on whitespace\n",
    "            trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency = 2) #only keep tokens that show up at least twice in the vocab\n",
    "            tokenizer.train_from_iterator(self.get_sentences(data, lang), trainer = trainer)\n",
    "            tokenizer.save(str(tokenizer_path))\n",
    "        else: \n",
    "            tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "        return tokenizer\n",
    "\n",
    "    def get_data(self):\n",
    "        seq_length = self.config[\"src_seq_length\"] #same for both languages\n",
    "        lang1 = self.config[\"lang1\"]\n",
    "        lang2 = self.config[\"lang2\"]\n",
    "        \n",
    "        data = load_dataset('opus_books', f'{lang1}-{lang2}', split = 'train') \n",
    "        #build tokenizers\n",
    "        tokenizer_src = self.build_tokenizer(data, lang1)\n",
    "        tokenizer_tgt = self.build_tokenizer(data, lang2)\n",
    "\n",
    "        #split for train and validation\n",
    "        train_data_size = int(0.9 * len(data)) \n",
    "        val_data_size = len(data) - train_data_size \n",
    "        train_data_raw, val_data_raw = random_split(data, [train_data_size, val_data_size]) #randomly split .9 for training, .1 for validation\n",
    "    \n",
    "        #process dataset using Dataset class from pytorch\n",
    "        train_data = ProcessDataset(train_data_raw, tokenizer_src, tokenizer_tgt, seq_length,lang1, lang2)\n",
    "        val_data = ProcessDataset(val_data_raw, tokenizer_src, tokenizer_tgt, seq_length, lang1, lang2)\n",
    "                                        \n",
    "        # Dataloaders iterate in batches\n",
    "        train_dataloader = DataLoader(train_data, batch_size = self.config['batch_size'], shuffle = True) \n",
    "        val_dataloader = DataLoader(val_data, batch_size = 1, shuffle = True)\n",
    "        \n",
    "        return train_dataloader,val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f2183645-addc-43a7-ad78-98b5d1d60d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildTransformer(Validation):\n",
    "    def __init__(self,data, config):\n",
    "        self.data = data\n",
    "        self.train_dataloader, self.val_dataloader, self.tokenizer_src, self.tokenizer_tgt = self.data.train_dataloader, self.data.val_dataloader, self.data.tokenizer_src, self.data.tokenizer_tgt\n",
    "        self.config = config\n",
    "        self.seq_length = self.config['src_seq_length'] #for src and tgt\n",
    "        \n",
    "        self.get_model()\n",
    "        #self.run_validation()\n",
    "\n",
    "    \n",
    "    def get_transformer(self):\n",
    "        N = self.config['N']\n",
    "        src_vocab_size = self.config['src_vocab_size']\n",
    "        tgt_vocab_size = self.config['tgt_vocab_size']\n",
    "        src_seq_length = self.config['src_seq_length']\n",
    "        tgt_seq_length = self.config['tgt_seq_length']\n",
    "        d_model = self.config['d_model']\n",
    "        h = self.config['h']\n",
    "        Pdrop = self.config['Pdrop']\n",
    "        \n",
    "        transformer = Transformer(N, src_vocab_size, tgt_vocab_size, src_seq_length, tgt_seq_length, d_model, h, Pdrop)\n",
    "       \n",
    "        for p in transformer.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "            return transformer\n",
    "\n",
    "    \n",
    "        \n",
    "    def get_model(self):\n",
    "        Path('model_weights').mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.config[\"src_vocab_size\"] = self.tokenizer_src.get_vocab_size()\n",
    "        self.config[\"tgt_vocab_size\"] = self.tokenizer_tgt.get_vocab_size()\n",
    "        self.model = self.get_transformer().to(device)\n",
    "    def computeLoss(self, proj_output, label, loss_fn,optimizer):\n",
    "        loss = loss_fn(proj_output.view(-1, self.tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # update parameters based on the gradients\n",
    "        optimizer.zero_grad() # clear gradients for net batch\n",
    "        return loss\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "        initial_epoch = 0\n",
    "        global_step = 0 # how many batches\n",
    "    \n",
    "        # check if model already pretrained, if so load model and state\n",
    "        if self.config['preload']:\n",
    "            epoch = self.config[\"preload_epoch\"]\n",
    "            model_filename =  f\"model_weights/weights_{epoch}.pt\"\n",
    "            state = torch.load(model_filename) \n",
    "            initial_epoch = state['epoch'] + 1\n",
    "            optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "            global_step = state['global_step']\n",
    "        \n",
    "        # cross entropy loss (L = - Σ_(for x in X) [y_x * log(p_x)])\n",
    "        # label smoothing, ignore padding when computing loss\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index = self.tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "        \n",
    "        for epoch in range(initial_epoch, self.config['num_epochs']):\n",
    "            self.model.train() #put into training mode (from model.eval())\n",
    "            n_seconds = 5\n",
    "            total_batches = len(self.train_dataloader)\n",
    "            print(f\"Number of Batches {total_batches}\")\n",
    "            batch_iterator = tqdm(transformer.train_dataloader, desc = f'epoch {epoch}',position=0, leave=True, mininterval=n_seconds)\n",
    "            i = 0\n",
    "            for batch in batch_iterator:\n",
    "                encoder_input = batch['encoder_input'].to(device)  #load data to device (GPU if available)\n",
    "                decoder_input = batch['decoder_input'].to(device)\n",
    "                encoder_mask = batch['encoder_mask'].to(device)\n",
    "                decoder_mask = batch['decoder_mask'].to(device)\n",
    "            \n",
    "                encoder_output = self.model.encode(encoder_input, encoder_mask)\n",
    "                decoder_output = self.model.decode(decoder_input,encoder_output, decoder_mask,encoder_mask)\n",
    "                proj_output = self.model.project(decoder_output)\n",
    "                \n",
    "\n",
    "               \n",
    "                label = batch['label'].to(device)  # load target labels to device\n",
    "                loss = self.computeLoss(proj_output,label, loss_fn,optimizer)\n",
    "                print(f\"Loss {loss}\")\n",
    "                batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})  #update progress bar\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # if i % 5 == 0: \n",
    "                #     batch_iterator.update(5)\n",
    "                i += 1\n",
    "            model_filename = f\"model_weights/weights_{epoch}.pt\"\n",
    "          \n",
    "            torch.save({\n",
    "                'epoch': epoch, \n",
    "                'model_state_dict': self.model.state_dict(),# Current model state\n",
    "                'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "                'global_step': global_step # Current global step \n",
    "            }, model_filename)\n",
    "            \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "493352e9-f367-4c54-a306-565354f84f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "            \"N\": 6,\n",
    "            \"src_seq_length\": 350,\n",
    "            \"tgt_seq_length\": 350,\n",
    "            \"d_model\": 512,\n",
    "            \"h\": 8,\n",
    "            \"Pdrop\": .1,\n",
    "            \"batch_size\": 8, #size 20 is 1455 batches\n",
    "            \"num_epochs\": 1,\n",
    "            \"lr\": 10**-4,\n",
    "            \"lang1\":\"en\",\n",
    "            \"lang2\": \"it\",\n",
    "            \"preload\":False,\n",
    "            \"preload_epoch\":0,\n",
    "            \"num_batches\":1\n",
    "        }\n",
    "# data = Data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7f03997-aa68-4d37-a597-e7ab4335742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f2736c3-db75-4ad4-b512-0d8b9c3343b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "transformer = BuildTransformer(data,config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "22f523cf-8009-4143-b110-69572310957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Batches 3638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:   0%|                 | 1/3638 [00:14<14:52:31, 14.72s/it, loss=10.018]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 10.017631530761719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:   0%|                  | 2/3638 [00:28<14:28:27, 14.33s/it, loss=9.992]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 9.992072105407715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:   0%|                  | 3/3638 [00:41<13:41:41, 13.56s/it, loss=9.952]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 9.951912879943848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:   0%|                  | 4/3638 [00:53<13:12:29, 13.08s/it, loss=9.904]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 9.903790473937988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:   0%|                  | 4/3638 [01:05<16:27:24, 16.30s/it, loss=9.904]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[75], line 84\u001b[0m, in \u001b[0;36mBuildTransformer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mproject(decoder_output)\n\u001b[1;32m     83\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# load target labels to device\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomputeLoss(proj_output,label, loss_fn,optimizer)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m batch_iterator\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m6.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m})  \u001b[38;5;66;03m#update progress bar\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[75], line 40\u001b[0m, in \u001b[0;36mBuildTransformer.computeLoss\u001b[0;34m(self, proj_output, label, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomputeLoss\u001b[39m(\u001b[38;5;28mself\u001b[39m, proj_output, label, loss_fn,optimizer):\n\u001b[1;32m     39\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(proj_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_tgt\u001b[38;5;241m.\u001b[39mget_vocab_size()), label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 40\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# update parameters based on the gradients\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# clear gradients for net batch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "de7b52b3-6bfc-443d-bf69-8950b7cbfe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: hello, output: manico convenire civili cantate pentiti torte scelti Potrei Potrei\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'manico convenire civili cantate pentiti torte scelti Potrei Potrei'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.predict(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f646dafc-12aa-4a91-b124-3cdac124ca88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "302ddb33-ecfb-423d-8018-93e951b967f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.tokenizer_src.encode(\"what\").ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "40292d47-76a4-4083-b84e-a4e18a8523d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2][:True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "480e77f3-2daa-49bc-a12f-480a844a59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(getattr(tqdm, '_instances'))\n",
    "\n",
    "for instance in list(tqdm._instances):\n",
    "    tqdm._decr_instances(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae15f95e-88eb-445c-bc04-38fe8fae6e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tqdm() Progress Bar:   0%|                     | 5/3638 [00:03<36:30,  1.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(transformer\u001b[38;5;241m.\u001b[39mtrain_dataloader, desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtqdm() Progress Bar\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "for i in tqdm(transformer.train_dataloader, desc = 'tqdm() Progress Bar'):\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2dafcc-4aab-40a6-9b3c-5f2a1c1af2df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
